{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import logging\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import compute_gap, preprocessing, extract_tfidf_keywords\n",
    "StopWords = set(stopwords.words('english'))\n",
    "porter = PorterStemmer()\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "Raw = [line.strip('\\n') for line in open('/home/hamish/Documents/datasets/1.Raw Input.txt').readlines()]\n",
    "\n",
    "True_cluster= pd.read_excel('/home/hamish/Documents/datasets/5.True Label (Ground Truth).xlsx',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'a',\n 'about',\n 'above',\n 'after',\n 'again',\n 'against',\n 'ain',\n 'all',\n 'am',\n 'an',\n 'and',\n 'any',\n 'are',\n 'aren',\n \"aren't\",\n 'as',\n 'at',\n 'be',\n 'because',\n 'been',\n 'before',\n 'being',\n 'below',\n 'between',\n 'both',\n 'but',\n 'by',\n 'can',\n 'couldn',\n \"couldn't\",\n 'd',\n 'did',\n 'didn',\n \"didn't\",\n 'do',\n 'does',\n 'doesn',\n \"doesn't\",\n 'doing',\n 'don',\n \"don't\",\n 'down',\n 'during',\n 'each',\n 'few',\n 'for',\n 'from',\n 'further',\n 'had',\n 'hadn',\n \"hadn't\",\n 'has',\n 'hasn',\n \"hasn't\",\n 'have',\n 'haven',\n \"haven't\",\n 'having',\n 'he',\n 'her',\n 'here',\n 'hers',\n 'herself',\n 'him',\n 'himself',\n 'his',\n 'how',\n 'i',\n 'if',\n 'in',\n 'into',\n 'is',\n 'isn',\n \"isn't\",\n 'it',\n \"it's\",\n 'its',\n 'itself',\n 'just',\n 'll',\n 'm',\n 'ma',\n 'me',\n 'mightn',\n \"mightn't\",\n 'more',\n 'most',\n 'mustn',\n \"mustn't\",\n 'my',\n 'myself',\n 'needn',\n \"needn't\",\n 'no',\n 'nor',\n 'not',\n 'now',\n 'o',\n 'of',\n 'off',\n 'on',\n 'once',\n 'only',\n 'or',\n 'other',\n 'our',\n 'ours',\n 'ourselves',\n 'out',\n 'over',\n 'own',\n 're',\n 's',\n 'same',\n 'shan',\n \"shan't\",\n 'she',\n \"she's\",\n 'should',\n \"should've\",\n 'shouldn',\n \"shouldn't\",\n 'so',\n 'some',\n 'such',\n 't',\n 'than',\n 'that',\n \"that'll\",\n 'the',\n 'their',\n 'theirs',\n 'them',\n 'themselves',\n 'then',\n 'there',\n 'these',\n 'they',\n 'this',\n 'those',\n 'through',\n 'to',\n 'too',\n 'under',\n 'until',\n 'up',\n 've',\n 'very',\n 'was',\n 'wasn',\n \"wasn't\",\n 'we',\n 'were',\n 'weren',\n \"weren't\",\n 'what',\n 'when',\n 'where',\n 'which',\n 'while',\n 'who',\n 'whom',\n 'why',\n 'will',\n 'with',\n 'won',\n \"won't\",\n 'wouldn',\n \"wouldn't\",\n 'y',\n 'you',\n \"you'd\",\n \"you'll\",\n \"you're\",\n \"you've\",\n 'your',\n 'yours',\n 'yourself',\n 'yourselves'}"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hamish/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StopWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-803635f906cd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdownload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'punkt'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mCleaned_Corpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mRaw\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mTF_Words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mextract_tfidf_keywords\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCleaned_Corpus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-8-803635f906cd>\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mnltk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdownload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'punkt'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mCleaned_Corpus\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mpreprocessing\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mline\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mRaw\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mTF_Words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mextract_tfidf_keywords\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCleaned_Corpus\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/projects/capstone/utils/utils.py\u001B[0m in \u001B[0;36mpreprocessing\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msub\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mr\"[{}]+\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpunctuation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m''\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0mtokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mword_tokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m     \u001B[0mfiltered_tokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mtoken\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtokens\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mStopWords\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mr'^(-?\\d+)(\\.\\d+)?$'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0;34m' '\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tokens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/projects/capstone/utils/utils.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     21\u001B[0m     \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msub\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mr\"[{}]+\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpunctuation\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m''\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[0mtokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mword_tokenize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlower\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 23\u001B[0;31m     \u001B[0mfiltered_tokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mtoken\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtokens\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mtoken\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mStopWords\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mre\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mr'^(-?\\d+)(\\.\\d+)?$'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtoken\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     24\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0;34m' '\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tokens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'StopWords' is not defined"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "Cleaned_Corpus = [preprocessing(line) for line in Raw]\n",
    "TF_Words = extract_tfidf_keywords(Cleaned_Corpus, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_Words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec([word_tokenize(line) for line in Cleaned_Corpus], size=100, window=5, min_count=1, workers=4)\n",
    "doc_vec = []\n",
    "for f in TF_Words:\n",
    "    doc_vec.append(np.mean(model[f], axis=0))\n",
    "X = pd.DataFrame(doc_vec)\n",
    "#X.to_excel('C:\\\\Users\\\\Owen\\\\Desktop\\\\4470 TF-IDF Filtered Version.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CBOW model\n",
    "    # train model\n",
    "CBOW_model = Word2Vec(TF_Words, min_count=1,size=100)  #CBOW\n",
    "#model = Word2Vec(texts, min_count=1,size=100,sg=1) #SG\n",
    "    # summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "#old model:Word2Vec(vocab=35455, size=50, alpha=0.025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_model.wv.index2entity[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results \n",
    "print(\"Cosine similarity between 'features' \" + \"and 'functions' - CBOW : \"\n",
    "      , CBOW_model.wv.similarity('features', 'functions')) \n",
    "print(\"Cosine similarity between 'science' \" + \"and 'technology' - CBOW : \"\n",
    "      , CBOW_model.wv.similarity('science', 'technology'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CBOW_model.most_similar('features')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, verbose=1, perplexity=2, n_iter=1000)\n",
    "digits_proj = tsne.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(digits_proj[:,0], digits_proj[:,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MeanShift\n",
    "import numpy as np\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=500)\n",
    "print(bandwidth)\n",
    "# 计算mean shift向量并进行聚类\n",
    "ms = MeanShift(bandwidth=0.2, bin_seeding=True)\n",
    "ms.fit(X)\n",
    "labels = ms.labels_\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "labels_unique = np.unique(labels)\n",
    "n_clusters_ = len(labels_unique)\n",
    "\n",
    "print(\"number of estimated clusters : %d\" % n_clusters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "#X = matrix_array\n",
    "db = DBSCAN(eps=0.1, min_samples=5).fit(X)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "objects = unique\n",
    "y_pos = unique\n",
    "performance =counts\n",
    "\n",
    "plt.bar(objects, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('clusters')\n",
    "plt.title('DBSCAN clusters distribution')\n",
    "\n",
    "for i in range(len(counts)):\n",
    "    plt.annotate(str(performance[i]), xy=(objects[i],performance[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=2, n_iter=1000)\n",
    "plt.figure(figsize=(60,10))\n",
    "for k in range(1,15):\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    a = kmeans.fit_predict(X)\n",
    "    plt.subplot(1,15,k)              #subplot( numRows, numCols,  plotNum)\n",
    "    \n",
    "    digits_proj = tsne.fit_transform(X)\n",
    "    plt.scatter(digits_proj[:,0], digits_proj[:,1], c=a)\n",
    "    #plt.scatter(x[:, 0], x[:, 1], c=a)\n",
    "    plt.xlabel('k='+str(k))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow method\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "#plt.plot()\n",
    "#X = np.array(list(zip(x1, x2))).reshape(len(x1), 2)\n",
    "plt.figure(figsize=(10,10))\n",
    "colors = ['b', 'g', 'r']\n",
    "markers = ['o', 'v', 's']\n",
    "\n",
    "# k means determine k\n",
    "distortions = []\n",
    "K = range(1,20)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "    kmeanModel.fit(X)\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_array=X.to_numpy()\n",
    "gap, reference_inertia, ondata_inertia = compute_gap(KMeans(),matrix_array)\n",
    "\n",
    "\n",
    "#plt.plot(range(1, 20+1), reference_inertia,'-o', label='abc')\n",
    "plt.plot(range(1, 20+1), ondata_inertia,\n",
    "         '-o', label='data')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('log(inertia)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(6, 6))\n",
    "#for k in range(1,13):\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "a = kmeans.fit_predict(X)\n",
    "#    plt.subplot(1,13,k)              #subplot( numRows, numCols,  plotNum)\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=2, n_iter=1000)\n",
    "digits_proj = tsne.fit_transform(X)\n",
    "plt.scatter(digits_proj[:,0], digits_proj[:,1], c=a)\n",
    "plt.xlabel('k='+str(10))\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(a, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "objects = unique\n",
    "y_pos = unique\n",
    "performance =counts\n",
    "\n",
    "plt.bar(objects, performance, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('clusters')\n",
    "plt.title('K-means clustering')\n",
    "\n",
    "for i in range(len(counts)):\n",
    "    plt.annotate(str(performance[i]), xy=(objects[i],performance[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "True_cluster_arr=True_cluster.to_numpy()\n",
    "True_cluster_arr=np.squeeze(True_cluster_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "True_cluster_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "True_cluster_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keman k=10\n",
    "jaccard(a[:10],True_cluster_arr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard(a,True_cluster_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FM\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score\n",
    "fowlkes_mallows_score(a , True_cluster_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11-a\n",
    "#01-b\n",
    "#10-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM(a[:10],True_cluster_arr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM(a,True_cluster_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#11-a\n",
    "#01-b\n",
    "#10-c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1(a,True_cluster_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1(a[:10],True_cluster_arr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "matrix_array=X.to_numpy()\n",
    "NUM_CLUSTERS=10\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(matrix_array, assign_clusters=True)\n",
    "print (assigned_clusters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard(assigned_clusters[:10],True_cluster_arr[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Quadratic-time MMD with Gaussian RBF kernel\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "def rbf_mmd2(X, Y, sigma=0, biased=True):\n",
    "    \n",
    "    gamma = 1 / (2 * sigma**2)\n",
    "\n",
    "    XX = T.dot(X, X.T)\n",
    "    XY = T.dot(X, Y.T)\n",
    "    YY = T.dot(Y, Y.T)\n",
    "\n",
    "    X_sqnorms = T.diagonal(XX)\n",
    "    Y_sqnorms = T.diagonal(YY)\n",
    "\n",
    "    K_XY = T.exp(-gamma * (\n",
    "            -2 * XY + X_sqnorms[:, np.newaxis] + Y_sqnorms[np.newaxis, :]))\n",
    "    K_XX = T.exp(-gamma * (\n",
    "            -2 * XX + X_sqnorms[:, np.newaxis] + X_sqnorms[np.newaxis, :]))\n",
    "    K_YY = T.exp(-gamma * (\n",
    "            -2 * YY + Y_sqnorms[:, np.newaxis] + Y_sqnorms[np.newaxis, :]))\n",
    "\n",
    "    if biased:\n",
    "        mmd2 = K_XX.mean() + K_YY.mean() - 2 * K_XY.mean()\n",
    "    else:\n",
    "        m = K_XX.shape[0]\n",
    "        n = K_YY.shape[0]\n",
    "\n",
    "        mmd2 = ((K_XX.sum() - m) / (m * (m - 1))\n",
    "              + (K_YY.sum() - n) / (n * (n - 1))\n",
    "              - 2 * K_XY.mean())\n",
    "    \n",
    "    return mmd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MMD(X,Y):\n",
    "    import theano\n",
    "    input1=np.reshape(X,(100,1))\n",
    "    input2=np.reshape(Y,(100,1))\n",
    "    Xth, Yth = T.matrices('X', 'Y')\n",
    "    sigmath = T.scalar('sigma')\n",
    "    fn = theano.function([Xth, Yth, sigmath],rbf_mmd2(Xth, Yth, sigma=sigmath))\n",
    "    mmd2 = fn(input1,input2, 1)\n",
    "    print(mmd2)\n",
    "    return float(mmd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_MMD(matrix_array[0],matrix_array[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "matrix_array=X.to_numpy()\n",
    "NUM_CLUSTERS=6\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(matrix_array, assign_clusters=True)\n",
    "print (assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(assigned_clusters, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard(assigned_clusters,True_cluster_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF02= pd.read_excel('L:\\\\09052020Topic 2 Dataset\\\\4.TF-IDF Filtered Version.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF02_arr=TF02.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer\n",
    "import nltk\n",
    "matrix_array=X.to_numpy()\n",
    "NUM_CLUSTERS=10\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(TF02_arr, assign_clusters=True)\n",
    "print (assigned_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard(assigned_clusters,True_cluster_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(assigned_clusters, return_counts=True)\n",
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-9f241b02",
   "language": "python",
   "display_name": "PyCharm (capstone)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}